{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages\n",
    "\n",
    "- string - to get the list of punctuation\n",
    "- re - to work with regular expressions\n",
    "- os - to work with operating system directory and files\n",
    "- itertools - to count frequencies\n",
    "- random - to shuffle lists\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - it is used to remove `HTML` tags\n",
    "------\n",
    "- [NumPy](http://www.numpy.org/) - to manipulate matrices\n",
    "- csv - to work with `csv` files\n",
    "- [matplotlib.pyplot](https://matplotlib.org/api/pyplot_api.html) - to work with plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import pickle as pkl\n",
    "from os import walk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Constants\n",
    "\n",
    "- `ARABIC_LETTERS_LIST` - a list containing the Arabic alphabet characters\n",
    "- `DIACRITICS_LIST` - a list containing the diacritics characters in Arabic language\n",
    "- `DN1` - directory name 1, for the processed data files\n",
    "- `DN2` - directory name 2, for the cleaned data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANTS_PATH = 'Constants'\n",
    "\n",
    "with open(CONSTANTS_PATH + '/ARABIC_LETTERS_LIST.pickle', 'rb') as file:\n",
    "    ARABIC_LETTERS_LIST = pkl.load(file)\n",
    "\n",
    "with open(CONSTANTS_PATH + '/DIACRITICS_LIST.pickle', 'rb') as file:\n",
    "    DIACRITICS_LIST = pkl.load(file)\n",
    "\n",
    "DN1 = 'processed'\n",
    "DN2 = 'cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Inputs\n",
    "\n",
    "- `dir_path` - directory path which containing the files to process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Clean the data\n",
    "\n",
    "1. Get files paths\n",
    "2. Helper methods\n",
    "3. Process the files\n",
    "\n",
    "## Cleaning process\n",
    "\n",
    "1. Remove `HTML` tags\n",
    "2. Remove URLs\n",
    "3. Fix diacritization issues\n",
    "4. Remove English letters\n",
    "5. Remove `Kashida` Arabic character\n",
    "6. Remove `*` (asterisk)\n",
    "7. Add space before and after the numbers\n",
    "8. Remove multiple whitespaces\n",
    "9. Remove diacritics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Get files paths\n",
    "\n",
    "- `files_paths` - a list containing the files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_paths = []\n",
    "for (dir_path, dirs_names, files_names) in walk(dir_path):\n",
    "    for file_name in files_names:\n",
    "        files_paths.append(dir_path + os.sep + file_name)\n",
    "print('Number of files:', len(files_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Helper methods\n",
    "\n",
    "- `clean(file_path)` - the main method that does the job\n",
    "- `read_file_content(file_path)` - reads the content of the file in `file_path`\n",
    "- `write_file_content(file_path, content, dir_name)` - writes `content` to file using `file_path` and `dir_name`\n",
    "- `remove_html_tags(content)` - removes the `HTML` tags from `content`\n",
    "- `remove_urls(content)` - removes the URLs from `content` using regular expressions\n",
    "- `fix_diacritics(content)` - fixes diacritics positions and remove unneeded or misplaced ones in `content`\n",
    "- `remove_english_letters(content)` - removes the English language letters from `content`\n",
    "- `remove_shift_j(content)` - removes `SHIFT+J` Arabic character from `content`\n",
    "- `remove_asterisk(content)` - removes `*` (asterisk) from `content`\n",
    "- `fix_numbers(content)` - add space before and after numbers in `content`\n",
    "- `remove_white_spaces(content)` - removes the white spaces from `content`\n",
    "- `remove_diacritics(content)` - removes the diacritics in `DIACRITICS_LIST` from `content`\n",
    "- `calculate_file_statistics(file_path, content, without_diac_content, split)` - calculates the statistics for the file in `file_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file_path):\n",
    "    print('Processing:', file_path)\n",
    "    \n",
    "    content = read_file_content(file_path)\n",
    "    \n",
    "    content = remove_html_tags(content)\n",
    "    content = remove_urls(content)\n",
    "    content = fix_diacritics(content)\n",
    "    content = remove_english_letters(content)\n",
    "    content = remove_shift_j(content)\n",
    "    content = fix_numbers(content)\n",
    "    content = remove_white_spaces(content)\n",
    "    without_diac_content = remove_diacritics(content)\n",
    "    \n",
    "    if len(content) == 0:\n",
    "        return ''\n",
    "    \n",
    "    write_file_content(file_path, content, DN1)\n",
    "    write_file_content(file_path, without_diac_content, DN2)\n",
    "    calculate_file_statistics(file_path, content, without_diac_content)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path):\n",
    "    return open(file_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_content(file_path, content, dir_name):\n",
    "    file_path = file_path.split(os.sep)\n",
    "    file_path = os.path.join(os.sep.join(file_path[:-1]), dir_name, file_path[-1])\n",
    "    print('Writing:', file_path)\n",
    "    with open(file_path, mode='w') as file_writer:\n",
    "        file_writer.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(content):\n",
    "    content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                     ' ', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'www(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                     ' ', content, flags=re.MULTILINE)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_diacritics(content):\n",
    "    content = re.sub(r'(?P<char>[' + ''.join(ARABIC_LETTERS_LIST) + DIACRITICS_LIST[-1] + '])\\s+(?P<diac>[' + ''.join(DIACRITICS_LIST) + ']+)(?P<brek>[\\s+]|\\Z)', r'\\g<char>\\g<diac>\\g<brek>', content)\n",
    "    content = re.sub(r'(?P<char>[^' + ''.join(ARABIC_LETTERS_LIST) + ''.join(DIACRITICS_LIST) + '])[' + ''.join(DIACRITICS_LIST) + ']+', r'\\g<char>', content)\n",
    "    content = re.sub(r'[' + DIACRITICS_LIST[-1] + ']+', DIACRITICS_LIST[-1], content)\n",
    "    content = re.sub(r'(?P<diac>[' + ''.join(DIACRITICS_LIST[:-1]) + '])[' + ''.join(DIACRITICS_LIST) + ']+', r'\\g<diac>', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_letters(content):\n",
    "    return content.translate(str.maketrans(string.ascii_letters, ' ' * len(string.ascii_letters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_shift_j(content):\n",
    "    return content.replace('Ù€', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_asterisk(content):\n",
    "    return content.replace('*', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_numbers(content):\n",
    "    return re.sub(r'(?P<numb>[0-9]+)', r' \\g<numb> ', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(content):\n",
    "    content = re.sub(r'[^\\S\\n]*\\n[\\s]*', '\\n', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'[^\\S\\n]+', ' ', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'\\A | \\Z', '', content, flags=re.MULTILINE)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(content):\n",
    "    return content.translate(str.maketrans('', '', ''.join(DIACRITICS_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_file_statistics(file_path, content, without_diac_content, split='training'):\n",
    "    content_splitted = content.split('\\n')\n",
    "    without_diac_content_splitted = without_diac_content.split('\\n')\n",
    "    file_statistics = dict()\n",
    "    \n",
    "    file_statistics['file_name'] = file_path.split('/')[-1]\n",
    "    \n",
    "    file_statistics['processed_chars_count'] = len(content)\n",
    "    file_statistics['cleaned_chars_count'] = len(without_diac_content)\n",
    "    file_statistics['words_count'] = 0\n",
    "    file_statistics['lines_count'] = len(without_diac_content_splitted)\n",
    "    \n",
    "    file_statistics['arabic_chars_count'] = 0\n",
    "    for ch in ARABIC_LETTERS_LIST:\n",
    "        file_statistics['arabic_chars_count'] += without_diac_content.count(ch)\n",
    "    \n",
    "    file_statistics['no_diacritics_percentage'] = 0\n",
    "    file_statistics['one_diacritics_percentage'] = 0\n",
    "    file_statistics['two_diacritics_percentage'] = 0\n",
    "    file_statistics['error_diacritics_percentage'] = 0\n",
    "    \n",
    "    for line in content_splitted:\n",
    "        tmp = 0\n",
    "        for word in line.split(' '):\n",
    "            if re.search(r'[' + ARABIC_LETTERS_LIST + ']+', word):\n",
    "                tmp += 1\n",
    "                file_statistics['words_count'] += 1\n",
    "                for idx, ch in enumerate(word):\n",
    "                    if ch in ARABIC_LETTERS_LIST:\n",
    "                        if idx + 1 >= len(word) or word[idx + 1] not in DIACRITICS_LIST:\n",
    "                            file_statistics['no_diacritics_percentage'] += 1\n",
    "                        elif word[idx + 1] == 'Ù‘':\n",
    "                            if idx + 2 >= len(word) or word[idx + 2] not in DIACRITICS_LIST:\n",
    "                                file_statistics['one_diacritics_percentage'] += 1\n",
    "                            elif word[idx + 2] != 'Ù‘' and (idx + 3 >= len(word) or word[idx + 3] not in DIACRITICS_LIST):\n",
    "                                file_statistics['two_diacritics_percentage'] += 1\n",
    "                            else:\n",
    "                                file_statistics['error_diacritics_percentage'] += 1\n",
    "                        else:\n",
    "                            if idx + 2 >= len(word) or word[idx + 2] not in DIACRITICS_LIST:\n",
    "                                file_statistics['one_diacritics_percentage'] += 1\n",
    "                            else:\n",
    "                                file_statistics['error_diacritics_percentage'] += 1\n",
    "        if split == 'training':\n",
    "            if tmp in line_lengths_fr.keys():\n",
    "                line_lengths_fr[tmp] += 1\n",
    "            else:\n",
    "                line_lengths_fr[tmp] = 1\n",
    "        elif split == 'testing':\n",
    "            if tmp in line_lengths_fr_testing.keys():\n",
    "                line_lengths_fr_testing[tmp] += 1\n",
    "            else:\n",
    "                line_lengths_fr_testing[tmp] = 1\n",
    "    \n",
    "    if file_statistics['words_count'] == 0:\n",
    "        return\n",
    "    \n",
    "    file_statistics['avg_word_chars'] = file_statistics['arabic_chars_count'] / file_statistics['words_count']\n",
    "    file_statistics['avg_line_words'] = file_statistics['words_count'] / len(without_diac_content_splitted)\n",
    "    \n",
    "    file_statistics['diacritics_count'] = file_statistics['processed_chars_count'] - file_statistics['cleaned_chars_count']\n",
    "    file_statistics['diacritics_percentage'] = (file_statistics['one_diacritics_percentage'] + file_statistics['two_diacritics_percentage']) / file_statistics['arabic_chars_count'] * 100\n",
    "    \n",
    "    assert(file_statistics['no_diacritics_percentage'] + file_statistics['one_diacritics_percentage'] + file_statistics['two_diacritics_percentage'] + file_statistics['error_diacritics_percentage'] == file_statistics['arabic_chars_count'])\n",
    "    \n",
    "    file_statistics['no_diacritics_percentage'] /= file_statistics['arabic_chars_count'] / 100\n",
    "    file_statistics['one_diacritics_percentage'] /= file_statistics['arabic_chars_count'] / 100\n",
    "    file_statistics['two_diacritics_percentage'] /= file_statistics['arabic_chars_count'] / 100\n",
    "    file_statistics['error_diacritics_percentage'] /= file_statistics['arabic_chars_count'] / 100\n",
    "    \n",
    "    file_statistics['min_line_chars'] = len(min(without_diac_content_splitted, key=len))\n",
    "    file_statistics['max_line_chars'] = len(max(without_diac_content_splitted, key=len))\n",
    "    file_statistics['min_line_words'] = len(min(without_diac_content_splitted, key=lambda line: len(line.split(' '))).split(' '))\n",
    "    file_statistics['max_line_words'] = len(max(without_diac_content_splitted, key=lambda line: len(line.split(' '))).split(' '))\n",
    "    \n",
    "    if split == 'training':\n",
    "        statistics.append(file_statistics)\n",
    "    elif split == 'testing':\n",
    "        statistics_testing.append(file_statistics)\n",
    "    \n",
    "    return file_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Process the files\n",
    "\n",
    "Call `clean` method on each file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(dir_path, DN1))\n",
    "os.mkdir(os.path.join(dir_path, DN2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = []\n",
    "line_lengths_fr = dict()\n",
    "statistics_testing = []\n",
    "line_lengths_fr_testing = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file_path in files_paths:\n",
    "    clean(file_path)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def write_dataset_statistics(output_file_name, statistics, line_lengths_fr):\n",
    "    with open(output_file_name + '.csv', 'w') as csv_writer:\n",
    "        f = csv.writer(csv_writer)\n",
    "\n",
    "        f.writerow(['file_name',\n",
    "                    'processed_chars_count', 'cleaned_chars_count',\n",
    "                    'words_count',\n",
    "                    'lines_count',\n",
    "                    'arabic_chars_count',\n",
    "                    'avg_word_chars', 'avg_line_words',\n",
    "                    'diacritics_count', 'diacritics_percentage',\n",
    "                    'no_diacritics_percentage', 'one_diacritics_percentage', 'two_diacritics_percentage', 'error_diacritics_percentage',\n",
    "                    'min_line_chars', 'max_line_chars',\n",
    "                    'min_line_words', 'max_line_words'])\n",
    "\n",
    "        for file_statistics in statistics:\n",
    "            f.writerow([file_statistics['file_name'],\n",
    "                        file_statistics['processed_chars_count'], file_statistics['cleaned_chars_count'],\n",
    "                        file_statistics['words_count'],\n",
    "                        file_statistics['lines_count'],\n",
    "                        file_statistics['arabic_chars_count'],\n",
    "                        file_statistics['avg_word_chars'], file_statistics['avg_line_words'],\n",
    "                        file_statistics['diacritics_count'], file_statistics['diacritics_percentage'],\n",
    "                        file_statistics['no_diacritics_percentage'], file_statistics['one_diacritics_percentage'], file_statistics['two_diacritics_percentage'], file_statistics['error_diacritics_percentage'],\n",
    "                        file_statistics['min_line_chars'], file_statistics['max_line_chars'],\n",
    "                        file_statistics['min_line_words'], file_statistics['max_line_words']])\n",
    "\n",
    "    figure(num=None, figsize=(8, 6), dpi=1000, facecolor='w', edgecolor='k')\n",
    "    plt.bar(line_lengths_fr.keys(), line_lengths_fr.values(), align='center')\n",
    "    plt.title(output_file_name + ' - Original')\n",
    "    plt.ylabel('Lines Lengths Frequency')\n",
    "    plt.xlabel('Lines Lengths')\n",
    "    plt.savefig(output_file_name + ' - Original')\n",
    "\n",
    "    figure(num=None, figsize=(8, 6), dpi=1000, facecolor='w', edgecolor='k')\n",
    "    plt.bar(line_lengths_fr.keys(), np.sqrt(np.sqrt(list(line_lengths_fr.values()))), align='center')\n",
    "    plt.title(output_file_name + ' - Re-Scaled')\n",
    "    plt.ylabel('Lines Lengths Frequency')\n",
    "    plt.xlabel('Lines Lengths')\n",
    "    plt.savefig(output_file_name + ' - Re-Scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset_statistics('Tashkeela Dataset Statistics', statistics, line_lengths_fr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
